{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37432bitd59c7cc563ec44edabfbbd54793ef745",
   "display_name": "Python 3.7.4 32-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline: Most Frequent Assigned Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most naive approach for PoS Tagging would be label a word with the most used label to that word in the corpus. This is the baseline approach that we are going to try in this notebook.\n",
    "\n",
    "The idea is load a treebank structured file (see how to create one in Reading, Saving and Loading PTbank files notebook) and get all nodes from the dataset. Then, assign to a dictionary the frequence table of occurrencies of labels for each word.\n",
    "\n",
    "```\n",
    "{\n",
    "    ...\n",
    "    'book': {\n",
    "        'NN': 256,\n",
    "        'VV': 126\n",
    "    },\n",
    "    'played': {\n",
    "        'VBN': 421\n",
    "    }\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "Take, for instance, the word book. To it could be assigned 'NN' (noun) in \"Where is my book?\" but also could be a 'VV' (verb, base form) \"I want to book a hotel\". The noun form is far more used than verb form, so if in an random unseen sentence it would be a good bet that the book word in it is a noun.\n",
    "\n",
    "At the end our frequence table will become a predictor with only the label more often for that symbol/word.\n",
    "\n",
    "```\n",
    "{\n",
    "    ...\n",
    "    'book': 'NN',\n",
    "    'played': 'VBN'\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use Treebank to load the structure and get the nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from postag import Treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "39831"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptb = Treebank()\n",
    "ptb.load('data/dumps/my_ptb_struct')\n",
    "len(ptb.instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, lets take all words-label tuple in the corpus, which are the nodes from each instance in the treebank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1014096"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = ptb.get_nodes()\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be sure if the predictor we are creating is good enought is interesting to split the samples in two groups: one for training, used to create the predictor table shown above, and another one for testing.\n",
    "\n",
    "The predictor maker is not going to see any test sample, so maybe new words appear, different labels for a know word, etc. The test set will be used to calculate the accuracy of the predictor, by running the predictions and counting the corrects and wrongs anwsers from the predictor.\n",
    "\n",
    "In order to split any unknown nodes we create the `split_data` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(nodes, train_perc, test_perc):\n",
    "    splitter = int(train_perc * len(nodes))\n",
    "    return nodes[:splitter], nodes[splitter:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split nodes in train set (with 70%) and test set (30%)\n",
    "train_set, test_set = split_data(nodes, 0.7, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the frequence table that counts the occurence of labels for a given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_table = {}\n",
    "\n",
    "# Generate the frequence table for classes / value tuple\n",
    "for node in train_set:\n",
    "    if node.value not in freq_table:\n",
    "        freq_table[node.value] = {}\n",
    "        freq_table[node.value][node.class_name] = 1\n",
    "    else:\n",
    "        if node.class_name in freq_table[node.value]:\n",
    "            freq_table[node.value][node.class_name] += 1\n",
    "        else:\n",
    "            freq_table[node.value][node.class_name] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'VB': 15, 'NN': 22, 'VBP': 1}"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_table['fight']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the freq table, it is time to pick the label with the most occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the most frequent class for each value\n",
    "for value in freq_table.keys():\n",
    "    label = max(iter(freq_table[value].keys()), key=(lambda key: freq_table[value][key]))\n",
    "    freq_table[value] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'NN'"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_table['fight']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. Our freq_table is now the predictor based on most frequents labels. Next, let's apply this predictor to the test set while counting the hits, wrongs and missing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the test evaluation\n",
    "hits = 0\n",
    "wrongs = 0\n",
    "misses = 0\n",
    "\n",
    "for node in test_set:\n",
    "    if node.value in freq_table:\n",
    "        predicted = freq_table[node.value]\n",
    "        if predicted == node.class_name:\n",
    "            hits += 1\n",
    "        else:\n",
    "            wrongs += 1\n",
    "    else:\n",
    "        misses += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the resulting accuracy for that experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "94.406750 Acurracy (277165 hits; 16421 wrongs)\nMissed 10643 values\n"
    }
   ],
   "source": [
    "print(\"%f Acurracy (%d hits; %d wrongs)\" % (((hits/(hits+wrongs))*100), hits, wrongs))\n",
    "print(\"Missed %d values\" % misses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this approach has a ~94% accuracy on PoS\n",
    "\n",
    "10,000 \"words\" are missing, that means they where not labeled because the predictor never saw them before. In this case I just ignored the missing words, but if we count a missing word as a wrong answer we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "91.104070 Acurracy (277165 hits; 16421 wrongs)\nMissed 10643 values\n"
    }
   ],
   "source": [
    "print(\"%f Acurracy (%d hits; %d wrongs)\" % (((hits/(hits+wrongs+misses))*100), hits, wrongs))\n",
    "print(\"Missed %d values\" % misses)"
   ]
  }
 ]
}